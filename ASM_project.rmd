---
output:
  html_document:
    toc: false
    toc_depth: '1'
    df_print: paged
  pdf_document:
    toc: false
    number_sections: false
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

\begin{titlepage}
    \centering
    \includegraphics[width=\textwidth]{UPC.jpg}
    \vfill
    {\Huge \textbf{ASM - Time Series Project}} \\[1.5cm]
    \vfill
    {\Large Authors: Dmitriy Chukhray, Julian Fransen} \\[0.5cm]
    {\Large Date: 2025-01-06}
\end{titlepage}

\newpage
\tableofcontents
\newpage

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
#setwd("~/uni_folder/ASM/asm_time_series/")
```
# Title page (julian will do this later when we finish)
# Table of contents (julian)
# Introduction
The dataset under analysis consists of monthly data on victims of traffic accidents in Spain, including fatalities, serious injuries, and minor injuries, recorded on urban and interurban roads. In this project we will apply the Box-Jenkins ARIMA methodology to understand the time-series dynamics of these traffic incidents and to make reliable predictions for future trends. Spanning from 1993 to 2019, the dataset captures over two decades of detailed information, offering a unique opportunity to identify trends, seasonal patterns, and underlying factors that influence traffic accidents.


# Analysis
## Load the time series data
```{r}
serie=ts(read.table("victimas.dat")/1000,start=1993,freq=12)
plot(serie, main="Victimas de Accidentes de Tráfico en España", ylab="Miles de Individuos")
abline(v=1992:2020,lty=3,col=4)
```

## Data transformations
```{r}
(m <- apply (matrix(serie[1:(27*12)], nr=12),2, mean)) # to check for constant variance
(v <- apply (matrix(serie[1:(27*12)], nr=12),2, var))
plot(v~m)
abline(lm(v~m),col=2,lty=3)

group <- c(rep(1:27, rep(12, 27)))
boxplot(serie ~ group,
        xlab = "Year",
        ylab = "Monthly values",
        main = "Check of constant variance (yearly boxplots)")
```

These plots show correct behaviour: `v` is basically uncorrelated with `m`, and the boxplots are similarily sized, implying constant variance. This means that a log transform is not necessary in our case. The next step is to check the existence of a seasonal pattern in the time series. To do that we are using the function monthplot.

```{r}
monthplot(serie)
```

In the plot above, we can clearly observe a seasonal pattern. If there were no seasonal component, the monthly means would remain at approximately the same level over time, and the shapes of the patterns for each month would not exhibit systematic repetition. These periodic fluctuations suggest that certain months consistently experience higher or lower values, driven by underlying seasonal component(s). To account for this seasonality, we apply a seasonal differencing transformation with a yearly period (12 months).

```{r}
d12serie=diff(serie,12)
plot(d12serie)
abline(h=0, col='red', lty=2)
```

The last step of achieving the stationary of the time series is to check whether the mean is constant or not. This can be done by examining the plot of the current time series data or by straight forwardly applying regular difference and then examining the change of the time series' variance.

```{r}
d1d12serie=diff(d12serie,1)
```

To verify if taking one regular difference is optimal, we calculate the variance of different transformation of the data:
1. original data (`serie`)
2. transformed with yearly season transformation (`d12serie`)
3. one regular difference applied to the previous transformation (`d1d12serie`)
4. another regular difference applied to the previous transformation

```{r}
#fix this output
var(serie)
var(d12serie)
var(d1d12serie) # our transformation
var(diff(d1d12serie)) # 2 regular differences
```

The total variance is minimal for one regular and one 12 month seasonal difference. This means that we should take one regular difference and no more. As we can see below, the data after transformations resembles white noise, which is the aim of data transformations in time-series data analysis. Now that we have stationary time series we can propose ARIMA models by evaluating ACF and PACF plots.

```{r}
plot(d1d12serie) 
abline(h=0, col='red', lty=2)
```

```{r, include=FALSE}
tserie <- d1d12serie #transformed serie is new name
```

```{r}
par(mfrow=c(1,2))
acf(tserie,ylim=c(-1,1),col=c(2,rep(1,11)),lwd=2,lag.max=84)
pacf(tserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=84)
```

To propose SARIMA models we need to evaluate ACF and PACF plots separately.
SARIMA model can be expressed as:

$$
SARIMA(p, d, q)(P, D, Q)_{s}
$$

Where:
- \( p, d, q \) represent the non-seasonal AR, differencing, and MA terms, respectively.
- \( P, D, Q \) represent the seasonal AR, differencing, and MA terms, respectively.
- \( s \) is the periodicity of the seasonal component.


Looking at the ACF plot we can clearly see decaying trend of non-seasonal lags after the lag 1, which is the biggest lag for the whole non-seasonal part and the subsequent lag experienced a sharp decline in ACF value hinting at possible MA(1) model (parameter q = 1). The same things can be said about seasonal lags in the ACF plot. There is a decaying trend of seasonal lags after the seasonal lag 1, which is the biggest seasonal lag for the whole seasonal part and the subsequent seasonal lag experienced a sharp decline in ACF value hinting at possible MA(1) model (parameter Q = 1). Now we move on to the PACF plot, which looks more complex than usual. The clearer trend is seen in seasonal lags of the PACF. The first seasonal lag is the biggest seasonal lag for the whole seasonal part and the subsequent seasonal lag experienced a sharp decline in PACF value hinting at possible AR(1) model (parameter P = 1). Here this trend is clearer than in the case of seasonal lags in the ACF because after seasonal lag 4 values become small so that exceeding confidence intervals becomes nearly impossible. In the ACF plot we have seen some seasonal lags being slightly above confidence intervals even though their antecedent lags were way below confidence intervals. The proposal of non-seasonal AR models by solely looking at the PACF plot in this particular case seems non-trivial, hence it would be a good idea to propose several hypothetical non-seasonal AR models and later check them using several model validation techniques. The decaying trend of non-seasonal AR part is there, though it is slower and seems to start later comparing it to non-seasonal MA part. Therefore, one could argue that one possible non-seasonal AR model which is AR(2) (parameter p = 2) because lag 2 is followed a lag that has experienced a sharp decline in PACF value. However, lags 4,5, and 6 are definitely above confidence intervals. Here is where we can propose another non-seasonal AR mode which is AR(6) (parameter p = 6), because lag 6 is followed by lags that are below confidence intervals consequentially and the decline in their values is sharper. Regarding the values of parameters d and D, we don't need to propose them because we know the actual values which are d = 1 and D = 1. In the end, combining everything we have seen we can propose 9 SARIMA models in the order of increasing complexity, which are:

$$
\text{1. } SARIMA(0, 1, 1)(1, 1, 0)_{12} \quad \text{(Non-seasonal MA(1), Seasonal AR(1))}
$$

$$
\text{2. } SARIMA(0, 1, 1)(0, 1, 1)_{12} \quad \text{(Non-seasonal MA(1), Seasonal MA(1))}
$$

$$
\text{3. } SARIMA(0, 1, 1)(1, 1, 1)_{12} \quad \text{(Non-seasonal MA(1), Seasonal AR(1), Seasonal MA(1))}
$$

$$
\text{4. } SARIMA(2, 1, 1)(1, 1, 0)_{12} \quad \text{(Non-seasonal MA(1), Non-seasonal AR(2), Seasonal AR(1))}
$$

$$
\text{5. } SARIMA(2, 1, 1)(0, 1, 1)_{12} \quad \text{(Non-seasonal MA(1), Non-seasonal AR(2), Seasonal MA(1))}
$$

$$
\text{6. } SARIMA(2, 1, 1)(1, 1, 1)_{12} \quad \text{(Non-seasonal MA(1), Non-seasonal AR(2), Seasonal AR(1), Seasonal MA(1))}
$$

$$
\text{7. } SARIMA(6, 1, 1)(1, 1, 0)_{12} \quad \text{(Non-seasonal MA(1), Non-seasonal AR(6), Seasonal AR(1))}
$$

$$
\text{8. } SARIMA(6, 1, 1)(0, 1, 1)_{12} \quad \text{(Non-seasonal MA(1), Non-seasonal AR(6), Seasonal MA(1))}
$$

$$
\text{9. } SARIMA(6, 1, 1)(1, 1, 1)_{12} \quad \text{(Non-seasonal MA(1), Non-seasonal AR(6), Seasonal AR(1), Seasonal MA(1))}
$$


## Validation
Now we will start model creation and validation. We use the `validation` function from the fourth practical of this course, as well as other pieces of code.

```{r, include=FALSE}
#################Validation#################################
validation=function(model){
  s=frequency(get(model$series))
  resid=model$residuals
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  #Residuals plot
  plot(resid,main="Residuals")
  abline(h=0)
  abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
  #Square Root of absolute values of residuals (Homocedasticity)
  scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals",
                 lpars=list(col=2))
  
  #Normal plot of residuals
  qqnorm(resid)
  qqline(resid,col=2,lwd=2)
  
  ##Histogram of residuals with normal curve
  hist(resid,breaks=20,freq=FALSE)
  curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)
  
  
  #ACF & PACF of residuals
  par(mfrow=c(1,2))
  acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
  pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
  par(mfrow=c(1,1))
  
  #Ljung-Box p-values
  par(mar=c(2,2,1,1))
  tsdiag(model,gof.lag=7*s)
  cat("\n--------------------------------------------------------------------\n")
  print(model)
  
  #Stationary and Invertible
  cat("\nModul of AR Characteristic polynomial Roots: ", 
      Mod(polyroot(c(1,-model$model$phi))),"\n")
  cat("\nModul of MA Characteristic polynomial Roots: ",
      Mod(polyroot(c(1,model$model$theta))),"\n")
  
  suppressMessages(require(forecast,quietly=TRUE,warn.conflicts=FALSE))
  plot(model)
  
  #Model expressed as an MA infinity (psi-weights)
  psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
  names(psis)=paste("psi",1:36)
  cat("\nPsi-weights (MA(inf))\n")
  cat("\n--------------------\n")
  print(psis[1:24])
  
  #Model expressed as an AR infinity (pi-weights)
  pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
  names(pis)=paste("pi",1:36)
  cat("\nPi-weights (AR(inf))\n")
  cat("\n--------------------\n")
  print(pis[1:24])
   
  cat("\nDescriptive Statistics for the Residuals\n")
  cat("\n----------------------------------------\n") 
  
  suppressMessages(require(fBasics,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(basicStats(resid))
  
  ## Add here complementary tests (use with caution!)
  ##---------------------------------------------------------
  cat("\nNormality Tests\n")
  cat("\n--------------------\n")
 
  ##Shapiro-Wilks Normality test
  print(shapiro.test(resid))

  suppressMessages(require(nortest,quietly=TRUE,warn.conflicts=FALSE))
  ##Anderson-Darling test
  print(ad.test(resid))
  
  suppressMessages(require(tseries,quietly=TRUE,warn.conflicts=FALSE))
  ##Jarque-Bera test
  print(jarque.bera.test(resid))
  
  cat("\nHomoscedasticity Test\n")
  cat("\n--------------------\n")
  suppressMessages(require(lmtest,quietly=TRUE,warn.conflicts=FALSE))
  ##Breusch-Pagan test
  obs=get(model$series)
  print(bptest(resid~I(obs-resid)))
  
  cat("\nIndependence Tests\n")
  cat("\n--------------------\n")
  
  ##Durbin-Watson test
  print(dwtest(resid~I(1:length(resid))))
  
  ##Ljung-Box test
  cat("\nLjung-Box test\n")
  print(t(apply(matrix(c(1:4,(1:4)*s)),1,function(el) {
    te=Box.test(resid,type="Ljung-Box",lag=el)
    c(lag=(te$parameter),statistic=te$statistic[[1]],p.value=te$p.value)})))
  
}
################# Fi Validation #################################
```


```{r, echo=FALSE}
# Initialize a data frame to store the results
results <- data.frame(p = integer(),
                      i = integer(),
                      q = integer(),
                      P = integer(),
                      D = integer(),
                      Q = integer(),
                      AIC = numeric(),
                      BIC = numeric())


# Loop over possible values of P, D, Q, and period
for (p in c(0,2,6)) {          # Experiment with seasonal AR orders (e.g., 0 to 2)
  for (P in c(0,1)) {        # Seasonal differencing (typically 0 or 1)
     for (Q in 0:1) {     # Experiment with seasonal MA orders (e.g., 0 to 2)
      

        # Fit the ARIMA model with the current seasonal parameters
        mod <- arima(serie, 
                     order = c(p, 1, 1), 
                     seasonal = list(order = c(P,1,Q), period = 12))

        # Store the AIC and BIC values
        results <- rbind(results, 
                            data.frame(p = p,
                                    i = 1,
                                    q = 1,
                                    P = P,
                                    D = 1,
                                    Q = Q, 
                                    AIC = AIC(mod), 
                                    BIC = BIC(mod)))
     }
  }
}

# Sort results by AIC (or BIC) for easier interpretation
dresults <- results[order(results$BIC), ]
# Display the results
print(dresults)
```
After experimenting with the `p`, `i`, and `q` parameters of `order`, and with the seasonality parameters as well, we have found 2 similarly performing models. `mod1` and `mod2` (see table below). Their value of `p` is different but all other parameters are identical. We chose these two models because they both have a low AIC value; `mod1` has the lowest and `mod2` has a slightly higher value, but has fewer parameters, reflected in the fact that is has the lowest BIC value, since this metric penalizes the number of parameters more than AIC. 

```{r, echo=FALSE}
(mod1=arima(tserie,order=c(0,1,1),seasonal=list(order=c(0,1,1),period=12)))
(mod2=arima(tserie,order=c(2,1,1),seasonal=list(order=c(0,1,1),period=12)))
(mod3=arima(tserie,order=c(2,1,1),seasonal=list(order=c(1,1,1),period=12)))
```

# okay dima, from here you go go
use echo=FALSE to not show the code; use include to not show the code AND output, use ```plaintext for text boxes. I will take care of the title page, table of contents etc. 

We will be taking a look at the residual plots for these models and selecting one for prediction.  


```{r}
validation(mod1)
```

```{r}
validation(mod2)
```

```{r}
validation(mod3)
```

```{r}
ultim=c(2018,12)
pdq=c(1,1,1)
PDQ=c(0,1,1)

serie2=window(serie,end=ultim)
lnserie2=log(serie2)
serie1=window(serie,end=ultim+c(1,0))
lnserie1=log(serie1)

(modA=arima(lnserie1,order=pdq,seasonal=list(order=PDQ,period=12)))
(modB=arima(lnserie2,order=pdq,seasonal=list(order=PDQ,period=12)))
```

```{r}
pred=predict(modB,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)
se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)


ts.plot(serie,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-2,+2),type="o",main=paste("Model ARIMA(",paste(pdq,collapse=","),")(",paste(PDQ,collapse=","),")12",sep=""))
abline(v=(ultim[1]-2):(ultim[1]+2),lty=3,col=4)
```

```{r}
pred=predict(modB,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)
se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)


ts.plot(serie,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-2,+2),type="o",main=paste("Model ARIMA(",paste(pdq,collapse=","),")(",paste(PDQ,collapse=","),")12",sep=""))
abline(v=(ultim[1]-2):(ultim[1]+2),lty=3,col=4)
```

```{r}
obs=window(serie,start=ultim+c(0,1))
pr=window(pr,start=ultim+c(0,1))
ts(data.frame(LowLim=tl[-1],Predic=pr,UpperLim=tu[-1],Observ=obs,Error=obs-pr,PercentError=(obs-pr)/obs),start=ultim+c(0,1),freq=12)
mod.RMSE1=sqrt(sum((obs-pr)^2)/12)
mod.MAE1=sum(abs(obs-pr))/12
mod.RMSPE1=sqrt(sum(((obs-pr)/obs)^2)/12)
mod.MAPE1=sum(abs(obs-pr)/obs)/12

data.frame("RMSE"=mod.RMSE1,"MAE"=mod.MAE1,"RMSPE"=mod.RMSPE1,"MAPE"=mod.MAPE1)

mCI1=mean(tu-tl)

cat("\nMean Length CI: ",mCI1)
```


```{r}
pred=predict(modA,n.ahead=12)
pr<-ts(c(tail(lnserie1,1),pred$pred),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

tl1<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu1<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr1<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl1,tu1,pr1,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=c(ultim[1]-2,ultim[1]+3),type="o",main=paste("Model ARIMA(",paste(pdq,collapse=","),")(",paste(PDQ,collapse=","),")12",sep=""))
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```


```{r}
(previs1=window(cbind(tl1,pr1,tu1),start=ultim+c(1,0)))
```

```{r}
ultim=c(2018,12)
pdq=c(8,1,0)
PDQ=c(0,1,1)

serie2=window(serie,end=ultim)
lnserie2=log(serie2)
serie1=window(serie,end=ultim+c(1,0))
lnserie1=log(serie1)

(modA=arima(lnserie1,order=pdq,seasonal=list(order=PDQ,period=12)))
(modB=arima(lnserie2,order=pdq,seasonal=list(order=PDQ,period=12)))
```

